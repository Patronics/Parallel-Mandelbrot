\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{listings}
\usepackage{appendix}

\parindent 0pt

\begin{document}

\title{\textbf{Parallelizing the Mandelbrot Set \\ \large Enhancing Performance through Multithreading and GPU Computing}}
\author{\begin{tabular}{cc}Patrick Leiser\\
Jared Push\\Catherine Yaroslavtseva\\\end{tabular}}

\date{June 2023}

\maketitle

\section{Abstract}

The Mandelbrot Set is a set of complex numbers (c) that exhibits intricate fractal patterns, given the parameter that $f_c(z) = z^2 + c$ where z is iterated from 0.
In this paper, we describe the investigation of various possible methods of optimizing the computation and visualization of the Mandelbrot Set, primarily through means of parallelization.
The primary purpose of these experiments is to explore different parallel programming environments by expanding on the base code provided by [INSERT].
We also focused on utilizing the strengths of GPUs and CPUs to find an optimal computational balance for various problem sizes.\\ 

The source code includes a serial version, an OpenMP version, a CUDA version, and various diverging versions of the latter two.

\section{Experiments}

This project involved a number of experiments, including but not limited to checking the viability of different CUDA block sizes, load distribution methods and mathematical optimizations.
While some changes were universal to all included implementations, the majority were isolated in order to accurately test their effects on the overall runtime.
Timing information is included within each relevant subsection, as well as a final analysis in the conclusion.
Effective changes were compiled into a cumulative final branch (main).

\subsection{Additional Features}

The contents of this section are universal to all implementations of the Mandelbrot Set discussed in this paper.
The original serial version of this program rendered monotonous fractals, with pixels distinguished by black, white and shades of gray.
The ability to zoom into the image or pan around it was also not included in the original program.
Thus, the first stage of this project involved adding coloration to the set and implementing zooming and panning functionality.\\

Colors in our implementation were calculated through simple multiplication and division operations.
Some versions included a colors struct and stored RGB values as 8-bit integers, but the general calculations remained the same:

\begin{verbatim}
int r = 255 * iter / maxiter;
int g = 255 * iter / (maxiter/30);
int b = 255 * iter / (maxiter/100);
\end{verbatim}

These calculations produced primarily blue and green shades for the exterior of the zoomed out version of the set, and yellow for the interiors of the bulbs and cardioids.
Ultimately, these color calculations were lower time-complexity than implementations that used trigonometric functions and created visually appealing Mandelbrot sets.\\

Zoom and pan functionality was added by listening specific keys of keyboard input, such as the 'i' and 'o' keys, with the former zooming in on the set and the latter zooming out on the set.
Similar to many video games, the panning is implemented with the 'w', 'a', 's', and 'd' keys, mimicking the layout of a keyboard's arrow keys.\\

There are 2 levels of zooming and panning, with finer adjustments achieved by using the shift key along with the appropriate input key.
Zoom and pan amounts can be further customized in the code as needed, and works by using the set distance or zoom ratio to transform the coordinates displayed on the screen.\\

The user can also restore the default position (showing the whole Mandelbrot set) with the 'r' key, or reflect the view using shift along with the 'R' key.

\subsection{Mathematical Optimization}

Initially, the sets rendered in parallel contained inconsistencies interference between thread color settings.This was expressed through incorrectly colored pixels rendered at various points of the set.
The issue was rectified by creating a critical section around the lines dedicated to drawing the pixels in the window.
This solution introduced overhead, which resulted in us decoupling the loops, and performing rendering after calculations were complete.\\

While implementing these changes, we found that the cpow and cabs operations were computationally expensive.
To help rectify this, and to fix compatibility issues with CUDA and complex numbers, we created an alternative version of the compute\_point() function.
The alternative function performs complex number calculations manually, rather than relying on C complex libraries.
It accomplishes this by splitting z into a real and imaginary component, then combining the components as needed for calculations.
During this process, we also experimented with using floats instead of doubles throughout the program, but found that this replacement caused rounding errors.
The rounding errors imposed limitations on the maximum extent of zoom functionality, so we decided to keep decimal data stored as double data types.\\

Mandelbrot sets are symmetrical across the x-axis.
We exploited this symmetry to avoid redundant calculations through mirroring.
This optimization is currently implemented in the cached CUDA version of the project.
This version computes the mirror value of the coordinate currently being processed, and caches the same color value for both the original coordinate and the mirrored coordinate.

\begin{verbatim}
flip_j = (height - my_j) + 1;
\end{verbatim}

When the mirror coordinate is processed, it is then able to fetch a color from the cache rather than recalculating it.
This modification to the cache reduced the required amount of calculations by half.

In addition to being symmetrical, Mandelbrot sets contain a large interior area that contains the same fill color.
 This area is contained within bulbs and cardioids.
  We found that the outlines of these areas are defined by the following equations:

\begin{verbatim}
Cardioid: cabs(1 - csqrt(1 - 4 * alpha)) <= 1
Bulb:     cabs(1 + alpha) <= 0.25
\end{verbatim}

The compute\_point() function initially checks these conditions to verify whether or not the point being computed is within a bulb or cardioid.
If the condition returns true, compute\_point() simply returns the max iteration value, thus saving time spent on calculations within the function.

\subsection{Scheduling and Distribution}

This results of this particular set of experiments varied between the OpenMP and CUDA implementations of the project.
While the OpenMP version performed multiple pixel computations per thread, the CUDA version performed most optimally with one pixel per thread.
Thus, this section will be split into an analysis of the OpenMP and CUDA experiments, respectively.\\

With OpenMP, we specifically tried dynamic and (static, 1) scheduling.
We also implemented an alternate cyclic distribution method, but found that dynamic produced the best results.\\

The results of our distribution experiments with CUDA are further elaborated on in subsection 2.6.
We experimented primarily with assigning varying amounts of pixels per thread.

\subsection{Load Balancing}

As the problem size, represented by the number of pixels requiring calculation, increases, the GPU demonstrates a time-saving advantage over both serial and parallel CPU implementations.
For smaller problem sizes, where the X11 window fits on a laptop or desktop monitor, a CPU that takes advantage of parallel programming paradigms is likely to outperform a GPU in pixel rendering.
However, for larger dimensions, GPUs will generally outperform CPUs.\\

We noticed this discrepancy and investigated ways to balance the workload by splitting it between the GPU and CPU.
Through various tests, we determined that with a problem size of over 300 million pixels placed between the bounds of (-1.5, -1.0) and (0.5, 1.0) with 3,000 iterations, combining the efforts of a GPU and CPU saved over 10 seconds compared to purely GPU computing.
Over 25 seconds were saved compared to the parallel CPU implementation, and several minutes compared to the naive serial implementation.
These results were attained by delegating the top 60 percent of the pixels to the GPU, and the bottom 40 percent of the pixels to the CPU.
These calculations were timed with an implementation that does not attempt to break out of loops early for (x,y) coordinates within a certain range.
These calculations were also not timed using an implementation that takes advantage of symmetry to avoid repeated computations.
In an implementation where the problem size of unique computations can be reduced, greater time improvements can be achieved by delegating more of the workload to the CPU.
The percentage of the workload assigned to the GPU and CPU can thus be determined dynamically based on the problem size.

\subsection{Multiple Kernels}

Another method we attempted to use to split the workload was to combine OpenMP and CUDA to launch multiple kernels, each with on own CPU thread.
In the process, we found that starting kernels in a parallelized for loop resulted in latency, and ultimately did not result in any time savings.
Thus, we chose not to pursue this method further.

\subsection{CUDA Block Sizes}

After debugging the CUDA implementation, we tested the efficiency of block sizes of 1, 16 and 32 with the following definitions in place: 

\begin{verbatim}
dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE); 
dim3 dimGrid(width/BLOCK_SIZE, height/BLOCK_SIZE);
\end{verbatim}

This was chosen based on a one pixel per thread implementation.
While we tested implementations that assigned multiple pixels per thread, we found that in the CUDA version, single pixel assignments facilitated the greatest speedups.

In the context of these definitions, we eventually settled on 16x16 block dimensions (256 threads), distributed across the grid in 1x1 blocks.
Initially, we had hypothesized that larger block sizes would produce better results, but found that a block size of 16 consistently generated the best times.

\subsection{Cache}

When applying transformations (such as vertical and horizontal shifts) to Mandelbrot sets, many of the computations are redundant.
To account for this, we implemented a cache to store the equivalent of a screenful of pixels that do not need to be recalculated.
For example, in the context of a 640x480 window, the cache stores a total of 307,200 pixel settings.
The cache is implemented as a struct, which stores an array of color struct objects.
While the image is being computed, the program is able to bypass the expensive compute\_point() function if a valid value is detected for the corresponding index in the cache.
Instead, it assigns the cached color value to the index currently being computed.
Since each index of the array represents a pixel of the screen, values are accessed by computing the one dimensional index equivalent, given the local i and j values of each thread, resulting in an O(1) cache access time.

\begin{verbatim}
index = my_i+width*my_j
\end{verbatim}

After transformations are applied, the local i and j values must be adjusted in order to access the correct index.
This is done by storing the accumulation of the transformations on each axis along with the coordinates, and calculating the adjusted i and j values by reversing these transformations.\\

In addition to the currently implemented cache format, we also explored a color-mapping cache.
Instead of storing the colors in association with the locations they were assigned to, this cache stores the colors associated with calculated values of "iter".
However, while this version of the cache requires significantly less storage, it requires a call to the compute\_point() function for each index, and thus results in significantly lower time savings.
This is especially true due to the overall low cost of our color calculations, so we decided not to include this cache implementation in the final version of the project.

\section{Conclusion}

In conclusion, this paper has done an in-depth analysis of the various optimizations of the Mandelbrot set problem, with an emphasis on parallelization.
We were able to achieve significant speedups through a culmination of leveraging CPU and GPU cores, caching, math optimization and distribution, and ultimately produced near real-time generations of Mandelbrot set visualizations.\\

While we have explored various methods, there is still potential for further research and testing to be done in order to achieve even greater performance gains.
Additionally, our implementation still lacks components such as mouse interaction, which could enhance its interactivity.

\pagebreak

\begin{center}

\section*{Citations}

\end{center}

\begin{verbatim}
Golla, Anudeep, and Paul Strode. “Discovery of the Heart in Mathematics: 
    Modeling the Chaotic Behaviors of Quantized Periods in the Mandelbrot Set: 
    Journal of Emerging Investigators.” Discovery of the Heart in Mathematics:
    Modeling the Chaotic Behaviors of Quantized Periods in the Mandelbrot Set | 
    Journal of Emerging Investigators, 14 Dec. 2020, emerginginvestigators.org/
    articles/discovery-of-the-heart-in-mathematics-modeling-the-chaotic-behaviors
    -of-quantized-periods-in-the-mandelbrot-set. 
    
Kraus, Jiri. “An Introduction to Cuda-Aware MPI.” NVIDIA Technical Blog, 21 
    Aug. 2022, developer.nvidia.com/blog/introduction-cuda-aware-mpi/. 
    
Latt, Jonas, et al. “Multi-Gpu Programming with Standard Parallel C++, Part 
    2.” NVIDIA Technical Blog, 26 May 2022, developer.nvidia.com/blog/
    multi-gpu-programming-with-standard-parallel-c-part-2/. 
    “Running Cuda-Aware Open MPI.” FAQ: Running Cuda-Aware Open MPI, www.open-
    mpi.org/faq/?category=runcuda. Accessed 13 June 2023. 

"Mandelbrot Test Code.” Mandelbrot Test Code - Optimizing CUDA for GPU 
    Architecture, selkie.macalester.edu/csinparallel/modules/CUDAArchitecture/
    build/html/1-Mandelbrot/Mandelbrot.html. Accessed 14 June 2023. 

Pacheco, Peter S., and Matthew Malensek. An Introduction to Parallel 
    Programming / Peter S. Pacheco, Matthew Malensek. Morgan Kaufmann 
    Publishers, an Imprint of Elsevier, 2022. 

Pacheco, Peter, and Matthew Malensek. “An Introduction to Parallel  
    Programming, 2nd Ed.” An Introduction to Parallel Programming, 2nd 
    Edition, 7 Mar. 2023, www.cs.usfca.edu/~peter/ipp2/index.html. 

Yi, X, et al. “Cudamicrobench: Microbenchmarks to Assist Cuda Performance
    Programming.” CUDAMicroBench: Microbenchmarks to Assist CUDA Performance 
    Programming (Conference) | OSTI.GOV, 27 Oct. 2021, www.osti.gov/servlets/purl/
    1828124. 
\end{verbatim}

\pagebreak

\begin{center}

\section*{Appendix}

\end{center}

\appendix

\section{Source Code}

\section{Contributions}

\subsection{Patrick Leiser}

\subsection{Jared Pugh}

\subsection{Catherine Yaroslavtseva}

\end{document}